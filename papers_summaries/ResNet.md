#### Deep Residual Learning for Image Recognition
ResNet is a convolutional neural network model that allows deeper architecture without degenerating the results (increasing the train error). The main idea of these networks is to fit the learning of the layers to a function that is a residual mapping (F(x)) instead of to an underlying mapping (H(x)). By this, we can express the underlying function as: H(x)=F(x)+x. Physically this is a shortcut in the structure. The advantage is that the solver -the part of the algorithm that train the network- optimize easily and faster that function, without degenerate the results. Using that approach we can have deeper networks with low training error and high detail in the descriptors, using from 18 layers to more than 500 or 1000 layers having good results. Still, we can suffer overfitting in the very deep architectures due to the features learned being too particular for the training set. 
In the paper the authors talked about a different block structure before the shortcut -how many convolutional layers and their size- and how to deal with the increasing dimension of the feature maps as in order to compute the residual the vectors or maps must have the same size. Authors propose to use a zero padding, the identity transform for the maps that have the same size and a projection shortcuts for those that have different size, or to use projection shortcuts for all the connections. At the end they present the results over different challenges showing a high performance on them.
